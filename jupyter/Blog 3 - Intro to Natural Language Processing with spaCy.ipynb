{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a natural language processing library for python. Some of its features are:\n",
    "- non-destructive tokenization, \n",
    "- named entity recognition, \n",
    "- pre-trained models \n",
    "- support for 52+ individual langueages. \n",
    "\n",
    "Since coding is best explained through examples, in this blog, we will use spacy to work through a book from [Project Gutenberg](http://www.gutenberg.org/ebooks/1661). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding proper nouns and custom patterns in The Adventures of Sherlock Holmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install spacy and language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in /home/anika/anaconda3/lib/python3.7/site-packages (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.2.0,>=7.1.1 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from thinc<7.2.0,>=7.1.1->spacy) (4.32.1)\n",
      "Requirement already satisfied: en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0 in /home/anika/anaconda3/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: spacy>=2.2.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.1.0)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.1.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.16.4)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/anika/anaconda3/lib/python3.7/site-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.32.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/anika/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model as the nlp onject\n",
    "\n",
    "spacy language and models can be downloaded as python packages. Some of the models and langugae packages can be found [here](https://spacy.io/usage/models). For this example, we will be using the small english languge web model 'en_core_web_sm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('data/1661-0.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the text is more than the default max length, so we reassign the max length of text to reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find sentences in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can access the sentences through a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A shock of orange hair, a\n",
       "pale face disfigured by a horrible scar, which, by its contraction, has\n",
       "turned up the outer edge of his upper lip, a bulldog chin, and a pair\n",
       "of very penetrating dark eyes, which present a singular contrast to the\n",
       "colour of his hair, all mark him out from amid the common crowd of\n",
       "mendicants and so, too, does his wit, for he is ever ready with a reply\n",
       "to any piece of chaff which may be thrown at him by the passers-by."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_sentences = [sentence for sentence in sentences if len(sentence) > 100]\n",
    "long_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization, Lemmatization and labeling of named entities\n",
    "\n",
    "When we call the nlp object on our text, it calls for a pipeline object that tokenizes, tags, parses and recognizes entities. We can access the tokens and entities through a loop, call for the lemmas of each word through .lemma_ and find labels for the entities through .label_\n",
    "\n",
    "![](https://d33wubrfki0l68.cloudfront.net/16b2ccafeefd6d547171afa23f9ac62f159e353d/48b91/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[problem, ., I, rang, the, bell, , and, was, shown]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in doc]\n",
    "token_list[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['problem', '.', '-PRON-', 'ring', 'the', 'bell', '\\n', 'and', 'be', 'show']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_list = [token.lemma_ for token in doc]\n",
    "lemma_list[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg : PERSON\n",
      "The Adventures of Sherlock Holmes : WORK_OF_ART\n",
      "Arthur Conan Doyle\n",
      "\n",
      " : ORG\n",
      "the Project Gutenberg License : ORG\n",
      "eBook : LAW\n",
      "The Adventures of Sherlock Holmes\n",
      "\n",
      " : WORK_OF_ART\n",
      "Arthur Conan Doyle\n",
      "\n",
      "Release Date : PERSON\n",
      "November 29, 2002 : DATE\n",
      "May 20, 2019 : DATE\n",
      "English : LANGUAGE\n"
     ]
    }
   ],
   "source": [
    "entity_list = []\n",
    "label_list = []\n",
    "for ent in doc.ents:\n",
    "    entity_list.append(ent) \n",
    "    label_list.append(ent.label_)\n",
    "i = 0\n",
    "while i < 10:\n",
    "    print(entity_list[i], ':', label_list[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's not perfect. Often times, for bigger text documents like this it is better to use larger models. 'en_core_web_sm' is a smaller model, as denoted by the 'sm' suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all the proper nouns in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proper nouns in text: 5070\n",
      "Example: ['Miss', 'Turner', 'James', 'Mr.', 'Holmes', 'Miss', 'Turner', 'God', 'Holmes', 'Lestrade', 'James', 'McCarthy', 'Holmes']\n"
     ]
    }
   ],
   "source": [
    "proper_noun_list = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "print('Number of proper nouns in text:', len(proper_noun_list))\n",
    "print('Example:', proper_noun_list[1537:1550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper nouns can also be accessed through loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words and punctuations can be targeted through is_stop and is_punct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'Adventures',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'Arthur',\n",
       " 'Conan',\n",
       " 'Doyle',\n",
       " '\\n\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_doc = [token.text for token in doc if (not token.is_stop) and (not token.is_punct)]\n",
    "clean_doc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our own stop_words or make a default stop_word not a stop_word with in the following [process](https://stackoverflow.com/questions/41170726/add-remove-stop-words-with-spacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenberg',\n",
       " 'Adventures',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'Arthur',\n",
       " 'Conan',\n",
       " 'Doyle',\n",
       " '\\n\\n',\n",
       " 'eBook']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"the\"].is_stop = False\n",
    "nlp.vocab[\"\\n\"].is_stop = True\n",
    "clean_text = [token.text for token in doc if (not token.is_stop) and (not token.is_punct)]\n",
    "clean_text[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dependencies and visualization through displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I wish she had been of my own station!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "      <th>related word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wish</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>had</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>been</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>my</td>\n",
       "      <td>PRON</td>\n",
       "      <td>poss</td>\n",
       "      <td>station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>own</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>station</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>!</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td></td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    pos    dep related word\n",
       "0         I   PRON  nsubj         wish\n",
       "1      wish   VERB   ROOT         wish\n",
       "2       she   PRON  nsubj         been\n",
       "3       had    AUX    aux         been\n",
       "4      been    AUX  ccomp         wish\n",
       "5        of    ADP   prep         been\n",
       "6        my   PRON   poss      station\n",
       "7       own    ADJ   amod      station\n",
       "8   station   NOUN   pobj           of\n",
       "9         !  PUNCT  punct         wish\n",
       "10       \\n  SPACE                   !"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the text and the predicted part-of-speech tag\n",
    "import pandas as pd\n",
    "print('Sentence:',sentences[637])\n",
    "sent_df = pd.DataFrame(columns = ['text', 'pos', 'dep', 'related word'])\n",
    "text_list = []\n",
    "pos_list = []\n",
    "dep_list = []\n",
    "rel_list = []\n",
    "for word in sentences[637]:\n",
    "    text_list.append(word.text)\n",
    "    pos_list.append(word.pos_)\n",
    "    dep_list.append(word.dep_)\n",
    "    rel_list.append(word.head.text)\n",
    "sent_df['text'] = text_list\n",
    "sent_df['pos'] = pos_list\n",
    "sent_df['dep'] = dep_list\n",
    "sent_df['related word'] = rel_list\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "# displacy.render(sentences[6751], style = 'dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the code block mentioned above is enough to render displacy in jupyter notebook, we need to use displacy.serve for other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here is what "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to extract every word group that consisted of a noun following an adjective, how would we do that?\n",
    "\n",
    "We can use Rule based matching to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "â€œâ€˜Jephro,â€™ said she, â€˜there is an impertinent fellow upon the road\n",
       "there who stares up at Miss Hunter.â€™\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[6751]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the matcher with shared vocab of the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom pattern and add it to the matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'POS': 'ADJ'}, \n",
    "           {'POS': 'NOUN', 'OP': '+'}]\n",
    "matcher.add('CUSTOM_PATTERN', None, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \"OP\" can have one of four values: \n",
    "- '!' for 0 times\n",
    "- '?' for 0 or 1 time\n",
    "- '+' for 1 or more times\n",
    "- '*' for 0 or more times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add pattern to the matcher as well thorugh matcher.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the matched spans through loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[other purposes, private note, own seal, little problem, serious one]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_list = [doc[start:end] for match_id, start, end in matches]\n",
    "match_list[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot more we can do with spacy, it is really well [documented](https://spacy.io/usage/spacy-101) and easy to pick up. So if you're looking to do some more NLP alongside what [nltk](https://www.nltk.org/) has to offer, check out spacy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
